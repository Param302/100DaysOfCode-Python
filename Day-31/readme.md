# Day 31 of [#100DaysOfCode](https://twitter.com/Param3021/status/1544261582565568512)

## Task
1. Titanic dataset prediction challenge

# Resources
- Kaggle [Titanic - Machine Learning from Disaster](https://www.kaggle.com/competitions/titanic/)
- - [My Notebook 1](https://www.kaggle.com/code/param302/titanic-survival-competition-2)
- - [My Notebook 2](https://www.kaggle.com/code/param302/titanic-survival-competition-3)


### Topics I have learnt
1. Titanic dataset prediction
- One with `RandomForestClassifier` and `OneHotEncoding`
- One with `XGBClassifier` and `OneHotEncoding`
- One with same as above but removed Outliers from data.

2. Also participated in MLH Global Hack Week INIT 2023 [ðŸ”—](https://ghw.mlh.io/)
- Did 1 challenge
- And tried to make discord bot but failed

### Software used
- Jupyter Notebook
- Python 3.10.2
- Numpy 1.22.4
- pandas 1.4.2
- Matplotlib 3.5.2
- Seaborn 0.11.2
- scikit-learn 1.1.1
- XGBoost 1.6.1

### My Notebooks
- [Titanic_survival_competition_2.ipynb](./Titanic_survival_competition_2.ipynb)
- [Titanic_survival_competition_3.ipynb](./Titanic_survival_competition_3.ipynb)
- [Titanic_survival_competition_4.ipynb](./Titanic_survival_competition_4.ipynb)

### Conclusion:
Today I did titanic survival compeition prediction 3 times, one time I scored better. I have used `XGBClassifier`, `RandomForestClassifier`.