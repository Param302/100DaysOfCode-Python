# Day 26 of [#100DaysOfCode](https://twitter.com/Param3021/status/1542445802865696770)

## Task
1. Clustering with K-means
2. PCA                      (only video)
3. House price prediction

# Resources
- Kaggle's [Feature Engineering Course](https://www.kaggle.com/learn/feature-engineering)
- - Lesson 4: [Clustering with K-means](https://www.kaggle.com/code/ryanholbrook/clustering-with-k-means) - [My Notebook](https://www.kaggle.com/code/param302/exercise-clustering-with-k-means/)

- Kaggle [House price prediction Challenge](https://www.kaggle.com/competitions/home-data-for-ml-course/)
- - [My Notebook 1](https://www.kaggle.com/param302/house-price-prediction-12)
- - [My Notebook 1](https://www.kaggle.com/param302/house-price-prediction-13)

### Topics I have learnt
1. Clustering with K-means
2. PCA
3. House price prediction
- One with Mutual Information and used `XGBRegressor`       (Score: 14900.48264)
- One with creating new features and used `XGBRegressor`    (Score: 15078.56818)

### Software used
- Jupyter Notebook
- Python 3.10.2
- Numpy 1.22.4
- pandas 1.4.2
- Matplotlib 3.5.2
- Seaborn 0.11.2
- scikit-learn 1.1.1
- XGBoost 1.6.1

### My Notebooks
- [House_price_prediction_12.ipynb](./House_price_prediction_12.ipynb)
- [House_price_prediction_13.ipynb](./House_price_prediction_13.ipynb)
- [L4 - Clustering_with_K-means.ipynb](./L4%20-%20Clustering_with_k-means.ipynb)

### Conclusion:
Today I learned about K-means clustering and PCA. Also did house price prediction with `XGBRegressor` and feature engineering.
